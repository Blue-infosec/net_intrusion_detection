{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "author=\"Jumabek Alikhanov\"\n",
    "date = 'Nov 19,2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "from os.path import join\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "#Please download dataset from http://205.174.165.80/CICDataset/CIC-IDS-2017/\n",
    "dataroot = 'MachineLearningCVE/'\n",
    "SEED=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MachineLearningCVE/*.pcap_ISCX.csv\n",
      "there are 2830743 flow records with 79 feature dimension\n",
      "stripped column names\n",
      "dropped bad columns\n",
      "There are 0 nan entries\n",
      "converted to numeric\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from preprocessing import load_data\n",
    "X,y = load_data(dataroot) # reads csv file and returns np array of X,y -> of shape (N,D) and (N,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imbalance\n",
    "1. It is curucial to adress this issue in order to get decent performance\n",
    "2. It also affects evaluation, we should calculate  `balanced accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import balance_data, normalize\n",
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%autoreload 2\n",
    "from models import Classifier\n",
    "\n",
    "def ensure_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "def getClassifier(args,runs_dir=None):\n",
    "    \n",
    "    (method,optim,lr,reg,batch_size,input_dim,num_class,num_epochs) = args\n",
    "    if runs_dir is not None:\n",
    "        ensure_dir(runs_dir)\n",
    "    \n",
    "    clf = Classifier(method,input_dim,num_class,lr=lr,reg=reg,num_epochs=num_epochs,\n",
    "                        batch_size=batch_size,runs_dir=runs_dir)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Fold #0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2264586, 76)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "K=5\n",
    "skf = StratifiedKFold(n_splits=K,random_state=SEED)\n",
    "for fold_index, (train_index,test_index) in enumerate(skf.split(X,y)):# runs only once \n",
    "        print('---------------------------------------------')\n",
    "        print('Fold #{}'.format(fold_index))    \n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 2.7035\n",
      "Epoch [1/20], Step [100/398], Loss: 2.6841\n",
      "Epoch [1/20], Step [150/398], Loss: 2.6640\n",
      "Epoch [1/20], Step [200/398], Loss: 2.6429\n",
      "Epoch [1/20], Step [250/398], Loss: 2.6241\n",
      "Epoch [1/20], Step [300/398], Loss: 2.6030\n",
      "Epoch [1/20], Step [350/398], Loss: 2.5855\n",
      "Epoch [2/20], Step [1/398], Loss: 2.5660\n",
      "Epoch [2/20], Step [51/398], Loss: 2.5475\n",
      "Epoch [2/20], Step [101/398], Loss: 2.5262\n",
      "Epoch [2/20], Step [151/398], Loss: 2.5121\n",
      "Epoch [2/20], Step [201/398], Loss: 2.4996\n",
      "Epoch [2/20], Step [251/398], Loss: 2.4747\n",
      "Epoch [2/20], Step [301/398], Loss: 2.4643\n",
      "Epoch [2/20], Step [351/398], Loss: 2.4435\n",
      "Epoch [3/20], Step [2/398], Loss: 2.4225\n",
      "Epoch [3/20], Step [52/398], Loss: 2.4107\n",
      "Epoch [3/20], Step [102/398], Loss: 2.3868\n",
      "Epoch [3/20], Step [152/398], Loss: 2.3747\n",
      "Epoch [3/20], Step [202/398], Loss: 2.3706\n",
      "Epoch [3/20], Step [252/398], Loss: 2.3569\n",
      "Epoch [3/20], Step [302/398], Loss: 2.3387\n",
      "Epoch [3/20], Step [352/398], Loss: 2.3201\n",
      "Epoch [4/20], Step [3/398], Loss: 2.3097\n",
      "Epoch [4/20], Step [53/398], Loss: 2.2948\n",
      "Epoch [4/20], Step [103/398], Loss: 2.2761\n",
      "Epoch [4/20], Step [153/398], Loss: 2.2533\n",
      "Epoch [4/20], Step [203/398], Loss: 2.2425\n",
      "Epoch [4/20], Step [253/398], Loss: 2.2347\n",
      "Epoch [4/20], Step [303/398], Loss: 2.2201\n",
      "Epoch [4/20], Step [353/398], Loss: 2.1978\n",
      "Epoch [5/20], Step [4/398], Loss: 2.1875\n",
      "Epoch [5/20], Step [54/398], Loss: 2.1785\n",
      "Epoch [5/20], Step [104/398], Loss: 2.1651\n",
      "Epoch [5/20], Step [154/398], Loss: 2.1619\n",
      "Epoch [5/20], Step [204/398], Loss: 2.1365\n",
      "Epoch [5/20], Step [254/398], Loss: 2.1317\n",
      "Epoch [5/20], Step [304/398], Loss: 2.1168\n",
      "Epoch [5/20], Step [354/398], Loss: 2.1033\n",
      "Epoch [6/20], Step [5/398], Loss: 2.0860\n",
      "Epoch [6/20], Step [55/398], Loss: 2.0860\n",
      "Epoch [6/20], Step [105/398], Loss: 2.0742\n",
      "Epoch [6/20], Step [155/398], Loss: 2.0464\n",
      "Epoch [6/20], Step [205/398], Loss: 2.0392\n",
      "Epoch [6/20], Step [255/398], Loss: 2.0509\n",
      "Epoch [6/20], Step [305/398], Loss: 2.0365\n",
      "Epoch [6/20], Step [355/398], Loss: 2.0120\n",
      "Epoch [7/20], Step [6/398], Loss: 1.9916\n",
      "Epoch [7/20], Step [56/398], Loss: 2.0128\n",
      "Epoch [7/20], Step [106/398], Loss: 1.9970\n",
      "Epoch [7/20], Step [156/398], Loss: 1.9827\n",
      "Epoch [7/20], Step [206/398], Loss: 1.9558\n",
      "Epoch [7/20], Step [256/398], Loss: 1.9610\n",
      "Epoch [7/20], Step [306/398], Loss: 1.9658\n",
      "Epoch [7/20], Step [356/398], Loss: 1.9481\n",
      "Epoch [8/20], Step [7/398], Loss: 1.9322\n",
      "Epoch [8/20], Step [57/398], Loss: 1.9209\n",
      "Epoch [8/20], Step [107/398], Loss: 1.9120\n",
      "Epoch [8/20], Step [157/398], Loss: 1.8966\n",
      "Epoch [8/20], Step [207/398], Loss: 1.9111\n",
      "Epoch [8/20], Step [257/398], Loss: 1.8772\n",
      "Epoch [8/20], Step [307/398], Loss: 1.8939\n",
      "Epoch [8/20], Step [357/398], Loss: 1.8674\n",
      "Epoch [9/20], Step [8/398], Loss: 1.8697\n",
      "Epoch [9/20], Step [58/398], Loss: 1.8543\n",
      "Epoch [9/20], Step [108/398], Loss: 1.8480\n",
      "Epoch [9/20], Step [158/398], Loss: 1.8415\n",
      "Epoch [9/20], Step [208/398], Loss: 1.8413\n",
      "Epoch [9/20], Step [258/398], Loss: 1.8182\n",
      "Epoch [9/20], Step [308/398], Loss: 1.8144\n",
      "Epoch [9/20], Step [358/398], Loss: 1.7887\n",
      "Epoch [10/20], Step [9/398], Loss: 1.7884\n",
      "Epoch [10/20], Step [59/398], Loss: 1.8018\n",
      "Epoch [10/20], Step [109/398], Loss: 1.7925\n",
      "Epoch [10/20], Step [159/398], Loss: 1.7663\n",
      "Epoch [10/20], Step [209/398], Loss: 1.7764\n",
      "Epoch [10/20], Step [259/398], Loss: 1.7884\n",
      "Epoch [10/20], Step [309/398], Loss: 1.7745\n",
      "Epoch [10/20], Step [359/398], Loss: 1.7415\n",
      "Epoch [11/20], Step [10/398], Loss: 1.7494\n",
      "Epoch [11/20], Step [60/398], Loss: 1.7324\n",
      "Epoch [11/20], Step [110/398], Loss: 1.7288\n",
      "Epoch [11/20], Step [160/398], Loss: 1.7217\n",
      "Epoch [11/20], Step [210/398], Loss: 1.7175\n",
      "Epoch [11/20], Step [260/398], Loss: 1.7069\n",
      "Epoch [11/20], Step [310/398], Loss: 1.7029\n",
      "Epoch [11/20], Step [360/398], Loss: 1.6888\n",
      "Epoch [12/20], Step [11/398], Loss: 1.7023\n",
      "Epoch [12/20], Step [61/398], Loss: 1.6826\n",
      "Epoch [12/20], Step [111/398], Loss: 1.6918\n",
      "Epoch [12/20], Step [161/398], Loss: 1.6586\n",
      "Epoch [12/20], Step [211/398], Loss: 1.6733\n",
      "Epoch [12/20], Step [261/398], Loss: 1.6463\n",
      "Epoch [12/20], Step [311/398], Loss: 1.6574\n",
      "Epoch [12/20], Step [361/398], Loss: 1.6270\n",
      "Epoch [13/20], Step [12/398], Loss: 1.6499\n",
      "Epoch [13/20], Step [62/398], Loss: 1.6405\n",
      "Epoch [13/20], Step [112/398], Loss: 1.6415\n",
      "Epoch [13/20], Step [162/398], Loss: 1.6228\n",
      "Epoch [13/20], Step [212/398], Loss: 1.6286\n",
      "Epoch [13/20], Step [262/398], Loss: 1.6215\n",
      "Epoch [13/20], Step [312/398], Loss: 1.6447\n",
      "Epoch [13/20], Step [362/398], Loss: 1.6159\n",
      "Epoch [14/20], Step [13/398], Loss: 1.6016\n",
      "Epoch [14/20], Step [63/398], Loss: 1.5920\n",
      "Epoch [14/20], Step [113/398], Loss: 1.6011\n",
      "Epoch [14/20], Step [163/398], Loss: 1.5869\n",
      "Epoch [14/20], Step [213/398], Loss: 1.5720\n",
      "Epoch [14/20], Step [263/398], Loss: 1.5717\n",
      "Epoch [14/20], Step [313/398], Loss: 1.5815\n",
      "Epoch [14/20], Step [363/398], Loss: 1.5605\n",
      "Epoch [15/20], Step [14/398], Loss: 1.5731\n",
      "Epoch [15/20], Step [64/398], Loss: 1.5551\n",
      "Epoch [15/20], Step [114/398], Loss: 1.5495\n",
      "Epoch [15/20], Step [164/398], Loss: 1.5551\n",
      "Epoch [15/20], Step [214/398], Loss: 1.5257\n",
      "Epoch [15/20], Step [264/398], Loss: 1.5452\n",
      "Epoch [15/20], Step [314/398], Loss: 1.5283\n",
      "Epoch [15/20], Step [364/398], Loss: 1.5444\n",
      "Epoch [16/20], Step [15/398], Loss: 1.5369\n",
      "Epoch [16/20], Step [65/398], Loss: 1.5133\n",
      "Epoch [16/20], Step [115/398], Loss: 1.5226\n",
      "Epoch [16/20], Step [165/398], Loss: 1.5147\n",
      "Epoch [16/20], Step [215/398], Loss: 1.5260\n",
      "Epoch [16/20], Step [265/398], Loss: 1.5026\n",
      "Epoch [16/20], Step [315/398], Loss: 1.5096\n",
      "Epoch [16/20], Step [365/398], Loss: 1.4975\n",
      "Epoch [17/20], Step [16/398], Loss: 1.4965\n",
      "Epoch [17/20], Step [66/398], Loss: 1.5026\n",
      "Epoch [17/20], Step [116/398], Loss: 1.4762\n",
      "Epoch [17/20], Step [166/398], Loss: 1.4871\n",
      "Epoch [17/20], Step [216/398], Loss: 1.4948\n",
      "Epoch [17/20], Step [266/398], Loss: 1.4777\n",
      "Epoch [17/20], Step [316/398], Loss: 1.4743\n",
      "Epoch [17/20], Step [366/398], Loss: 1.4628\n",
      "Epoch [18/20], Step [17/398], Loss: 1.4534\n",
      "Epoch [18/20], Step [67/398], Loss: 1.4547\n",
      "Epoch [18/20], Step [117/398], Loss: 1.4493\n",
      "Epoch [18/20], Step [167/398], Loss: 1.4536\n",
      "Epoch [18/20], Step [217/398], Loss: 1.4556\n",
      "Epoch [18/20], Step [267/398], Loss: 1.4597\n",
      "Epoch [18/20], Step [317/398], Loss: 1.4449\n",
      "Epoch [18/20], Step [367/398], Loss: 1.4034\n",
      "Epoch [19/20], Step [18/398], Loss: 1.4380\n",
      "Epoch [19/20], Step [68/398], Loss: 1.4210\n",
      "Epoch [19/20], Step [118/398], Loss: 1.4145\n",
      "Epoch [19/20], Step [168/398], Loss: 1.4134\n",
      "Epoch [19/20], Step [218/398], Loss: 1.4360\n",
      "Epoch [19/20], Step [268/398], Loss: 1.4406\n",
      "Epoch [19/20], Step [318/398], Loss: 1.4003\n",
      "Epoch [19/20], Step [368/398], Loss: 1.3982\n",
      "Epoch [20/20], Step [19/398], Loss: 1.3901\n",
      "Epoch [20/20], Step [69/398], Loss: 1.4074\n",
      "Epoch [20/20], Step [119/398], Loss: 1.3972\n",
      "Epoch [20/20], Step [169/398], Loss: 1.4028\n",
      "Epoch [20/20], Step [219/398], Loss: 1.4066\n",
      "Epoch [20/20], Step [269/398], Loss: 1.3955\n",
      "Epoch [20/20], Step [319/398], Loss: 1.3934\n",
      "Epoch [20/20], Step [369/398], Loss: 1.3686\n",
      "Model is trained in 737 sec \n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 2.6707\n",
      "Epoch [1/20], Step [100/398], Loss: 2.6525\n",
      "Epoch [1/20], Step [150/398], Loss: 2.6325\n",
      "Epoch [1/20], Step [200/398], Loss: 2.6110\n",
      "Epoch [1/20], Step [250/398], Loss: 2.5945\n",
      "Epoch [1/20], Step [300/398], Loss: 2.5740\n",
      "Epoch [1/20], Step [350/398], Loss: 2.5521\n",
      "Epoch [2/20], Step [1/398], Loss: 2.5358\n",
      "Epoch [2/20], Step [51/398], Loss: 2.5145\n",
      "Epoch [2/20], Step [101/398], Loss: 2.4992\n",
      "Epoch [2/20], Step [151/398], Loss: 2.4848\n",
      "Epoch [2/20], Step [201/398], Loss: 2.4730\n",
      "Epoch [2/20], Step [251/398], Loss: 2.4489\n",
      "Epoch [2/20], Step [301/398], Loss: 2.4386\n",
      "Epoch [2/20], Step [351/398], Loss: 2.4208\n",
      "Epoch [3/20], Step [2/398], Loss: 2.3994\n",
      "Epoch [3/20], Step [52/398], Loss: 2.3905\n",
      "Epoch [3/20], Step [102/398], Loss: 2.3768\n",
      "Epoch [3/20], Step [152/398], Loss: 2.3542\n",
      "Epoch [3/20], Step [202/398], Loss: 2.3465\n",
      "Epoch [3/20], Step [252/398], Loss: 2.3310\n",
      "Epoch [3/20], Step [302/398], Loss: 2.3199\n",
      "Epoch [3/20], Step [352/398], Loss: 2.2915\n",
      "Epoch [4/20], Step [3/398], Loss: 2.2876\n",
      "Epoch [4/20], Step [53/398], Loss: 2.2611\n",
      "Epoch [4/20], Step [103/398], Loss: 2.2589\n",
      "Epoch [4/20], Step [153/398], Loss: 2.2457\n",
      "Epoch [4/20], Step [203/398], Loss: 2.2267\n",
      "Epoch [4/20], Step [253/398], Loss: 2.2198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Step [303/398], Loss: 2.2057\n",
      "Epoch [4/20], Step [353/398], Loss: 2.1714\n",
      "Epoch [5/20], Step [4/398], Loss: 2.1746\n",
      "Epoch [5/20], Step [54/398], Loss: 2.1562\n",
      "Epoch [5/20], Step [104/398], Loss: 2.1477\n",
      "Epoch [5/20], Step [154/398], Loss: 2.1573\n",
      "Epoch [5/20], Step [204/398], Loss: 2.1141\n",
      "Epoch [5/20], Step [254/398], Loss: 2.1221\n",
      "Epoch [5/20], Step [304/398], Loss: 2.1033\n",
      "Epoch [5/20], Step [354/398], Loss: 2.0927\n",
      "Epoch [6/20], Step [5/398], Loss: 2.0764\n",
      "Epoch [6/20], Step [55/398], Loss: 2.0745\n",
      "Epoch [6/20], Step [105/398], Loss: 2.0593\n",
      "Epoch [6/20], Step [155/398], Loss: 2.0408\n",
      "Epoch [6/20], Step [205/398], Loss: 2.0246\n",
      "Epoch [6/20], Step [255/398], Loss: 2.0168\n",
      "Epoch [6/20], Step [305/398], Loss: 2.0311\n",
      "Epoch [6/20], Step [355/398], Loss: 2.0044\n",
      "Epoch [7/20], Step [6/398], Loss: 1.9947\n",
      "Epoch [7/20], Step [56/398], Loss: 1.9770\n",
      "Epoch [7/20], Step [106/398], Loss: 1.9763\n",
      "Epoch [7/20], Step [156/398], Loss: 1.9729\n",
      "Epoch [7/20], Step [206/398], Loss: 1.9476\n",
      "Epoch [7/20], Step [256/398], Loss: 1.9537\n",
      "Epoch [7/20], Step [306/398], Loss: 1.9607\n",
      "Epoch [7/20], Step [356/398], Loss: 1.9319\n",
      "Epoch [8/20], Step [7/398], Loss: 1.9158\n",
      "Epoch [8/20], Step [57/398], Loss: 1.9099\n",
      "Epoch [8/20], Step [107/398], Loss: 1.9127\n",
      "Epoch [8/20], Step [157/398], Loss: 1.9113\n",
      "Epoch [8/20], Step [207/398], Loss: 1.8978\n",
      "Epoch [8/20], Step [257/398], Loss: 1.8700\n",
      "Epoch [8/20], Step [307/398], Loss: 1.8736\n",
      "Epoch [8/20], Step [357/398], Loss: 1.8430\n",
      "Epoch [9/20], Step [8/398], Loss: 1.8632\n",
      "Epoch [9/20], Step [58/398], Loss: 1.8463\n",
      "Epoch [9/20], Step [108/398], Loss: 1.8411\n",
      "Epoch [9/20], Step [158/398], Loss: 1.8323\n",
      "Epoch [9/20], Step [208/398], Loss: 1.8287\n",
      "Epoch [9/20], Step [258/398], Loss: 1.8131\n",
      "Epoch [9/20], Step [308/398], Loss: 1.7979\n",
      "Epoch [9/20], Step [358/398], Loss: 1.7806\n",
      "Epoch [10/20], Step [9/398], Loss: 1.7858\n",
      "Epoch [10/20], Step [59/398], Loss: 1.7878\n",
      "Epoch [10/20], Step [109/398], Loss: 1.7846\n",
      "Epoch [10/20], Step [159/398], Loss: 1.7538\n",
      "Epoch [10/20], Step [209/398], Loss: 1.7745\n",
      "Epoch [10/20], Step [259/398], Loss: 1.7466\n",
      "Epoch [10/20], Step [309/398], Loss: 1.7557\n",
      "Epoch [10/20], Step [359/398], Loss: 1.7146\n",
      "Epoch [11/20], Step [10/398], Loss: 1.7486\n",
      "Epoch [11/20], Step [60/398], Loss: 1.7311\n",
      "Epoch [11/20], Step [110/398], Loss: 1.7306\n",
      "Epoch [11/20], Step [160/398], Loss: 1.7143\n",
      "Epoch [11/20], Step [210/398], Loss: 1.6993\n",
      "Epoch [11/20], Step [260/398], Loss: 1.7106\n",
      "Epoch [11/20], Step [310/398], Loss: 1.6928\n",
      "Epoch [11/20], Step [360/398], Loss: 1.6750\n",
      "Epoch [12/20], Step [11/398], Loss: 1.6730\n",
      "Epoch [12/20], Step [61/398], Loss: 1.6714\n",
      "Epoch [12/20], Step [111/398], Loss: 1.6683\n",
      "Epoch [12/20], Step [161/398], Loss: 1.6693\n",
      "Epoch [12/20], Step [211/398], Loss: 1.6670\n",
      "Epoch [12/20], Step [261/398], Loss: 1.6436\n",
      "Epoch [12/20], Step [311/398], Loss: 1.6450\n",
      "Epoch [12/20], Step [361/398], Loss: 1.6313\n",
      "Epoch [13/20], Step [12/398], Loss: 1.6465\n",
      "Epoch [13/20], Step [62/398], Loss: 1.6385\n",
      "Epoch [13/20], Step [112/398], Loss: 1.6233\n",
      "Epoch [13/20], Step [162/398], Loss: 1.6139\n",
      "Epoch [13/20], Step [212/398], Loss: 1.6382\n",
      "Epoch [13/20], Step [262/398], Loss: 1.6013\n",
      "Epoch [13/20], Step [312/398], Loss: 1.6315\n",
      "Epoch [13/20], Step [362/398], Loss: 1.5978\n",
      "Epoch [14/20], Step [13/398], Loss: 1.6066\n",
      "Epoch [14/20], Step [63/398], Loss: 1.5972\n",
      "Epoch [14/20], Step [113/398], Loss: 1.5987\n",
      "Epoch [14/20], Step [163/398], Loss: 1.5956\n",
      "Epoch [14/20], Step [213/398], Loss: 1.5759\n",
      "Epoch [14/20], Step [263/398], Loss: 1.5669\n",
      "Epoch [14/20], Step [313/398], Loss: 1.5842\n",
      "Epoch [14/20], Step [363/398], Loss: 1.5532\n",
      "Epoch [15/20], Step [14/398], Loss: 1.5623\n",
      "Epoch [15/20], Step [64/398], Loss: 1.5685\n",
      "Epoch [15/20], Step [114/398], Loss: 1.5478\n",
      "Epoch [15/20], Step [164/398], Loss: 1.5513\n",
      "Epoch [15/20], Step [214/398], Loss: 1.5311\n",
      "Epoch [15/20], Step [264/398], Loss: 1.5543\n",
      "Epoch [15/20], Step [314/398], Loss: 1.5310\n",
      "Epoch [15/20], Step [364/398], Loss: 1.5382\n",
      "Epoch [16/20], Step [15/398], Loss: 1.5259\n",
      "Epoch [16/20], Step [65/398], Loss: 1.5166\n",
      "Epoch [16/20], Step [115/398], Loss: 1.5313\n",
      "Epoch [16/20], Step [165/398], Loss: 1.5343\n",
      "Epoch [16/20], Step [215/398], Loss: 1.5062\n",
      "Epoch [16/20], Step [265/398], Loss: 1.5096\n",
      "Epoch [16/20], Step [315/398], Loss: 1.5039\n",
      "Epoch [16/20], Step [365/398], Loss: 1.4914\n",
      "Epoch [17/20], Step [16/398], Loss: 1.4752\n",
      "Epoch [17/20], Step [66/398], Loss: 1.4991\n",
      "Epoch [17/20], Step [116/398], Loss: 1.4805\n",
      "Epoch [17/20], Step [166/398], Loss: 1.4740\n",
      "Epoch [17/20], Step [216/398], Loss: 1.4892\n",
      "Epoch [17/20], Step [266/398], Loss: 1.4831\n",
      "Epoch [17/20], Step [316/398], Loss: 1.4580\n",
      "Epoch [17/20], Step [366/398], Loss: 1.4673\n",
      "Epoch [18/20], Step [17/398], Loss: 1.4510\n",
      "Epoch [18/20], Step [67/398], Loss: 1.4762\n",
      "Epoch [18/20], Step [117/398], Loss: 1.4542\n",
      "Epoch [18/20], Step [167/398], Loss: 1.4259\n",
      "Epoch [18/20], Step [217/398], Loss: 1.4519\n",
      "Epoch [18/20], Step [267/398], Loss: 1.4555\n",
      "Epoch [18/20], Step [317/398], Loss: 1.4342\n",
      "Epoch [18/20], Step [367/398], Loss: 1.4308\n",
      "Epoch [19/20], Step [18/398], Loss: 1.4064\n",
      "Epoch [19/20], Step [68/398], Loss: 1.4258\n",
      "Epoch [19/20], Step [118/398], Loss: 1.3984\n",
      "Epoch [19/20], Step [168/398], Loss: 1.4335\n",
      "Epoch [19/20], Step [218/398], Loss: 1.4157\n",
      "Epoch [19/20], Step [268/398], Loss: 1.4231\n",
      "Epoch [19/20], Step [318/398], Loss: 1.4132\n",
      "Epoch [19/20], Step [368/398], Loss: 1.4104\n",
      "Epoch [20/20], Step [19/398], Loss: 1.4181\n",
      "Epoch [20/20], Step [69/398], Loss: 1.4096\n",
      "Epoch [20/20], Step [119/398], Loss: 1.3718\n",
      "Epoch [20/20], Step [169/398], Loss: 1.3917\n",
      "Epoch [20/20], Step [219/398], Loss: 1.3731\n",
      "Epoch [20/20], Step [269/398], Loss: 1.3876\n",
      "Epoch [20/20], Step [319/398], Loss: 1.3819\n",
      "no improvement in accuracy for 10 iterations\n",
      "Model is trained in 726 sec \n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 2.6919\n",
      "Epoch [1/20], Step [100/398], Loss: 2.6706\n",
      "Epoch [1/20], Step [150/398], Loss: 2.6547\n",
      "Epoch [1/20], Step [200/398], Loss: 2.6331\n",
      "Epoch [1/20], Step [250/398], Loss: 2.6127\n",
      "Epoch [1/20], Step [300/398], Loss: 2.5941\n",
      "Epoch [1/20], Step [350/398], Loss: 2.5750\n",
      "Epoch [2/20], Step [1/398], Loss: 2.5596\n",
      "Epoch [2/20], Step [51/398], Loss: 2.5422\n",
      "Epoch [2/20], Step [101/398], Loss: 2.5213\n",
      "Epoch [2/20], Step [151/398], Loss: 2.5114\n",
      "Epoch [2/20], Step [201/398], Loss: 2.4920\n",
      "Epoch [2/20], Step [251/398], Loss: 2.4737\n",
      "Epoch [2/20], Step [301/398], Loss: 2.4639\n",
      "Epoch [2/20], Step [351/398], Loss: 2.4384\n",
      "Epoch [3/20], Step [2/398], Loss: 2.4267\n",
      "Epoch [3/20], Step [52/398], Loss: 2.4123\n",
      "Epoch [3/20], Step [102/398], Loss: 2.3930\n",
      "Epoch [3/20], Step [152/398], Loss: 2.3791\n",
      "Epoch [3/20], Step [202/398], Loss: 2.3732\n",
      "Epoch [3/20], Step [252/398], Loss: 2.3599\n",
      "Epoch [3/20], Step [302/398], Loss: 2.3466\n",
      "Epoch [3/20], Step [352/398], Loss: 2.3342\n",
      "Epoch [4/20], Step [3/398], Loss: 2.3171\n",
      "Epoch [4/20], Step [53/398], Loss: 2.2928\n",
      "Epoch [4/20], Step [103/398], Loss: 2.2892\n",
      "Epoch [4/20], Step [153/398], Loss: 2.2744\n",
      "Epoch [4/20], Step [203/398], Loss: 2.2627\n",
      "Epoch [4/20], Step [253/398], Loss: 2.2486\n",
      "Epoch [4/20], Step [303/398], Loss: 2.2479\n",
      "Epoch [4/20], Step [353/398], Loss: 2.2161\n",
      "Epoch [5/20], Step [4/398], Loss: 2.2129\n",
      "Epoch [5/20], Step [54/398], Loss: 2.1925\n",
      "Epoch [5/20], Step [104/398], Loss: 2.1917\n",
      "Epoch [5/20], Step [154/398], Loss: 2.1730\n",
      "Epoch [5/20], Step [204/398], Loss: 2.1649\n",
      "Epoch [5/20], Step [254/398], Loss: 2.1615\n",
      "Epoch [5/20], Step [304/398], Loss: 2.1516\n",
      "Epoch [5/20], Step [354/398], Loss: 2.1373\n",
      "Epoch [6/20], Step [5/398], Loss: 2.1239\n",
      "Epoch [6/20], Step [55/398], Loss: 2.1066\n",
      "Epoch [6/20], Step [105/398], Loss: 2.1105\n",
      "no improvement in accuracy for 10 iterations\n",
      "Model is trained in 210 sec \n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 1.6801\n",
      "Epoch [1/20], Step [100/398], Loss: 1.3891\n",
      "Epoch [1/20], Step [150/398], Loss: 1.2009\n",
      "Epoch [1/20], Step [200/398], Loss: 1.0549\n",
      "Epoch [1/20], Step [250/398], Loss: 1.0046\n",
      "Epoch [1/20], Step [300/398], Loss: 0.9077\n",
      "Epoch [1/20], Step [350/398], Loss: 0.8743\n",
      "Epoch [2/20], Step [1/398], Loss: 0.8220\n",
      "Epoch [2/20], Step [51/398], Loss: 0.7902\n",
      "Epoch [2/20], Step [101/398], Loss: 0.7293\n",
      "Epoch [2/20], Step [151/398], Loss: 0.7156\n",
      "Epoch [2/20], Step [201/398], Loss: 0.7119\n",
      "Epoch [2/20], Step [251/398], Loss: 0.6462\n",
      "Epoch [2/20], Step [301/398], Loss: 0.6482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Step [351/398], Loss: 0.6268\n",
      "Epoch [3/20], Step [2/398], Loss: 0.6061\n",
      "Epoch [3/20], Step [52/398], Loss: 0.6357\n",
      "Epoch [3/20], Step [102/398], Loss: 0.5767\n",
      "Epoch [3/20], Step [152/398], Loss: 0.5872\n",
      "Epoch [3/20], Step [202/398], Loss: 0.5769\n",
      "Epoch [3/20], Step [252/398], Loss: 0.5627\n",
      "Epoch [3/20], Step [302/398], Loss: 0.5750\n",
      "Epoch [3/20], Step [352/398], Loss: 0.5534\n",
      "Epoch [4/20], Step [3/398], Loss: 0.5417\n",
      "Epoch [4/20], Step [53/398], Loss: 0.5320\n",
      "Epoch [4/20], Step [103/398], Loss: 0.5502\n",
      "Epoch [4/20], Step [153/398], Loss: 0.5403\n",
      "Epoch [4/20], Step [203/398], Loss: 0.5182\n",
      "Epoch [4/20], Step [253/398], Loss: 0.5280\n",
      "Epoch [4/20], Step [303/398], Loss: 0.5319\n",
      "Epoch [4/20], Step [353/398], Loss: 0.5183\n",
      "Epoch [5/20], Step [4/398], Loss: 0.4884\n",
      "Epoch [5/20], Step [54/398], Loss: 0.4891\n",
      "Epoch [5/20], Step [104/398], Loss: 0.5065\n",
      "Epoch [5/20], Step [154/398], Loss: 0.4940\n",
      "Epoch [5/20], Step [204/398], Loss: 0.4803\n",
      "Epoch [5/20], Step [254/398], Loss: 0.4850\n",
      "Epoch [5/20], Step [304/398], Loss: 0.4882\n",
      "Epoch [5/20], Step [354/398], Loss: 0.4893\n",
      "Epoch [6/20], Step [5/398], Loss: 0.4679\n",
      "Epoch [6/20], Step [55/398], Loss: 0.4734\n",
      "Epoch [6/20], Step [105/398], Loss: 0.4516\n",
      "Epoch [6/20], Step [155/398], Loss: 0.4735\n",
      "Epoch [6/20], Step [205/398], Loss: 0.4579\n",
      "Epoch [6/20], Step [255/398], Loss: 0.4691\n",
      "Epoch [6/20], Step [305/398], Loss: 0.4900\n",
      "Epoch [6/20], Step [355/398], Loss: 0.4576\n",
      "Epoch [7/20], Step [6/398], Loss: 0.4582\n",
      "Epoch [7/20], Step [56/398], Loss: 0.4507\n",
      "Epoch [7/20], Step [106/398], Loss: 0.4498\n",
      "Epoch [7/20], Step [156/398], Loss: 0.4536\n",
      "Epoch [7/20], Step [206/398], Loss: 0.4355\n",
      "Epoch [7/20], Step [256/398], Loss: 0.4295\n",
      "Epoch [7/20], Step [306/398], Loss: 0.4566\n",
      "Epoch [7/20], Step [356/398], Loss: 0.4460\n",
      "Epoch [8/20], Step [7/398], Loss: 0.4445\n",
      "Epoch [8/20], Step [57/398], Loss: 0.4615\n",
      "Epoch [8/20], Step [107/398], Loss: 0.4247\n",
      "Epoch [8/20], Step [157/398], Loss: 0.4228\n",
      "Epoch [8/20], Step [207/398], Loss: 0.4313\n",
      "Epoch [8/20], Step [257/398], Loss: 0.4528\n",
      "Epoch [8/20], Step [307/398], Loss: 0.4382\n",
      "Epoch [8/20], Step [357/398], Loss: 0.4438\n",
      "Epoch [9/20], Step [8/398], Loss: 0.4438\n",
      "Epoch [9/20], Step [58/398], Loss: 0.4290\n",
      "Epoch [9/20], Step [108/398], Loss: 0.4422\n",
      "Epoch [9/20], Step [158/398], Loss: 0.4185\n",
      "Epoch [9/20], Step [208/398], Loss: 0.4279\n",
      "Epoch [9/20], Step [258/398], Loss: 0.4440\n",
      "Epoch [9/20], Step [308/398], Loss: 0.4201\n",
      "Epoch [9/20], Step [358/398], Loss: 0.4309\n",
      "Epoch [10/20], Step [9/398], Loss: 0.4251\n",
      "Epoch [10/20], Step [59/398], Loss: 0.4173\n",
      "Epoch [10/20], Step [109/398], Loss: 0.4431\n",
      "Epoch [10/20], Step [159/398], Loss: 0.4287\n",
      "Epoch [10/20], Step [209/398], Loss: 0.4151\n",
      "Epoch [10/20], Step [259/398], Loss: 0.4131\n",
      "Epoch [10/20], Step [309/398], Loss: 0.4411\n",
      "Epoch [10/20], Step [359/398], Loss: 0.4238\n",
      "Epoch [11/20], Step [10/398], Loss: 0.4355\n",
      "Epoch [11/20], Step [60/398], Loss: 0.4060\n",
      "Epoch [11/20], Step [110/398], Loss: 0.4309\n",
      "Epoch [11/20], Step [160/398], Loss: 0.4358\n",
      "Epoch [11/20], Step [210/398], Loss: 0.4368\n",
      "Epoch [11/20], Step [260/398], Loss: 0.3922\n",
      "Epoch [11/20], Step [310/398], Loss: 0.4207\n",
      "Epoch [11/20], Step [360/398], Loss: 0.3807\n",
      "Epoch [12/20], Step [11/398], Loss: 0.4242\n",
      "Epoch [12/20], Step [61/398], Loss: 0.4121\n",
      "Epoch [12/20], Step [111/398], Loss: 0.4228\n",
      "Epoch [12/20], Step [161/398], Loss: 0.4016\n",
      "Epoch [12/20], Step [211/398], Loss: 0.4008\n",
      "Epoch [12/20], Step [261/398], Loss: 0.3978\n",
      "no improvement in accuracy for 10 iterations\n",
      "Model is trained in 439 sec \n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 1.7014\n",
      "Epoch [1/20], Step [100/398], Loss: 1.3939\n",
      "Epoch [1/20], Step [150/398], Loss: 1.2028\n",
      "Epoch [1/20], Step [200/398], Loss: 1.0816\n",
      "Epoch [1/20], Step [250/398], Loss: 1.0132\n",
      "Epoch [1/20], Step [300/398], Loss: 0.9451\n",
      "Epoch [1/20], Step [350/398], Loss: 0.8853\n",
      "Epoch [2/20], Step [1/398], Loss: 0.8484\n",
      "Epoch [2/20], Step [51/398], Loss: 0.8267\n",
      "Epoch [2/20], Step [101/398], Loss: 0.7894\n",
      "Epoch [2/20], Step [151/398], Loss: 0.7750\n",
      "Epoch [2/20], Step [201/398], Loss: 0.7737\n",
      "Epoch [2/20], Step [251/398], Loss: 0.7495\n",
      "Epoch [2/20], Step [301/398], Loss: 0.7539\n",
      "Epoch [2/20], Step [351/398], Loss: 0.7425\n",
      "Epoch [3/20], Step [2/398], Loss: 0.7158\n",
      "Epoch [3/20], Step [52/398], Loss: 0.7412\n",
      "no improvement in accuracy for 10 iterations\n",
      "Model is trained in 101 sec \n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 1.8074\n",
      "Epoch [1/20], Step [100/398], Loss: 1.7216\n",
      "Epoch [1/20], Step [150/398], Loss: 1.6712\n",
      "Epoch [1/20], Step [200/398], Loss: 1.6478\n",
      "Epoch [1/20], Step [250/398], Loss: 1.6502\n",
      "Epoch [1/20], Step [300/398], Loss: 1.6549\n",
      "Epoch [1/20], Step [350/398], Loss: 1.6328\n",
      "Epoch [2/20], Step [1/398], Loss: 1.6411\n",
      "Epoch [2/20], Step [51/398], Loss: 1.6430\n",
      "Epoch [2/20], Step [101/398], Loss: 1.6390\n",
      "Epoch [2/20], Step [151/398], Loss: 1.6380\n",
      "no improvement in accuracy for 10 iterations\n",
      "Model is trained in 74 sec \n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 0.4847\n",
      "Epoch [1/20], Step [100/398], Loss: 0.4399\n",
      "Epoch [1/20], Step [150/398], Loss: 0.4327\n",
      "Epoch [1/20], Step [200/398], Loss: 0.4146\n",
      "Epoch [1/20], Step [250/398], Loss: 0.4243\n",
      "Epoch [1/20], Step [300/398], Loss: 0.3945\n",
      "Epoch [1/20], Step [350/398], Loss: 0.3911\n",
      "Epoch [2/20], Step [1/398], Loss: 0.4353\n",
      "Epoch [2/20], Step [51/398], Loss: 0.3741\n",
      "Epoch [2/20], Step [101/398], Loss: 0.3753\n",
      "Epoch [2/20], Step [151/398], Loss: 0.3791\n",
      "Epoch [2/20], Step [201/398], Loss: 0.4087\n",
      "Epoch [2/20], Step [251/398], Loss: 0.3750\n",
      "Epoch [2/20], Step [301/398], Loss: 0.3867\n",
      "Epoch [2/20], Step [351/398], Loss: 0.3921\n",
      "Epoch [3/20], Step [2/398], Loss: 0.4039\n",
      "Epoch [3/20], Step [52/398], Loss: 0.3941\n",
      "Epoch [3/20], Step [102/398], Loss: 0.3766\n",
      "no improvement in accuracy for 10 iterations\n",
      "Model is trained in 105 sec \n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 0.7253\n",
      "Epoch [1/20], Step [100/398], Loss: 0.7143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#hyper-params\n",
    "batch_size = 5120\n",
    "optim = 'Adam'\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "num_class = len(np.unique(y_train))\n",
    "\n",
    "accuracies = {}\n",
    "learning_rates = [1e-4,1e-2,1e-0]\n",
    "regularizations = [1e-6,1e-4,1e-2]\n",
    "best_model = None\n",
    "best_acc = -1\n",
    "num_layers = 3\n",
    "method = 'softmax'\n",
    "num_epochs = 20\n",
    "for lr in learning_rates:\n",
    "    for reg in regularizations:\n",
    "\n",
    "        classifier_args = (method,optim,lr,reg,batch_size,input_dim,num_class,num_epochs)\n",
    "        config =  '{}/train/optim_{}_lr_{}_reg_{}_bs_{}'.format(method,num_layers,optim,lr,reg,batch_size)\n",
    "        runs_dir = join(dataroot,'runs',config)\n",
    "        \n",
    "        X_train = X_train.astype(float)\n",
    "        y_train = y_train.astype(int)\n",
    "        p = np.random.permutation(len(y_train))\n",
    "        X_train = X_train[p]\n",
    "        y_train = y_train[p]\n",
    "        X_train,y_train = balance_data(X_train,y_train,seed=SEED)\n",
    "\n",
    "        tick = time.time()\n",
    "        clf = getClassifier(classifier_args,runs_dir)\n",
    "        \n",
    "        clf.fit(X_train,y_train)\n",
    "        pred = clf.predict(X_test)\n",
    "        \n",
    "        acc = metrics.balanced_accuracy_score(y_test,pred)\n",
    "        if acc >best_acc:\n",
    "            best_model = clf\n",
    "            best_acc = acc\n",
    "        accuracies[(lr,reg)]=acc\n",
    "        tock = time.time()\n",
    "        print(\"Model is trained in {0:.0f} sec \".format(tock-tick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-validation results\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "results = accuracies\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot validation accuracy\n",
    "marker_size=100\n",
    "colors = [results[x] for x in results] # default size of markers is 20\n",
    "\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('Net intrusion(CIC-IDS-2017) validation accuracy')\n",
    "plt.savefig(join(dataroot,'runs',loss_function,'1st_run.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
