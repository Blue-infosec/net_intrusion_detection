{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "author=\"Jumabek Alikhanov\"\n",
    "date = 'Nov 17,2019'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Final Project material for the \"Deep Learning\" class I took  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset from https://www.unb.ca/cic/datasets/ids-2017.html\n",
    "We only need CSV files that is preprocessed and labeled for ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "from os.path import join\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataroot = 'MachineLearningCVE/'\n",
    "SEED =2 \n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MachineLearningCVE/*.pcap_ISCX.csv\n",
      "there are 2830743 flow records with 79 feature dimension\n",
      "stripped column names\n",
      "dropped bad columns\n",
      "There are 0 nan entries\n",
      "converted to numeric\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "from preprocessing import load_data\n",
    "X,y = load_data(dataroot) # reads csv file and returns np array of X,y -> of shape (N,D) and (N,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imbalance\n",
    "1. It is curucial to adress this issue in order to get decent performance\n",
    "2. It also affects evaluation, we should calculate  `balanced accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import balance_data, normalize\n",
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%autoreload 2\n",
    "from models import Classifier\n",
    "\n",
    "def ensure_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "def getClassifier(args,runs_dir=None):\n",
    "    \n",
    "    (method,optim,lr,reg,batch_size,input_dim,num_class,num_iters) = args\n",
    "    if runs_dir is not None:\n",
    "        ensure_dir(runs_dir)\n",
    "    \n",
    "    clf = Classifier(method,input_dim,num_class,lr=lr,reg=reg,num_epochs=num_iters,\n",
    "                        batch_size=batch_size,runs_dir=runs_dir)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "def Kfold_evaluation(X,y,method='nn3',K=5,lr=1e-3,reg=1e-5,num_epochs=20):\n",
    "    results = {}\n",
    "\n",
    "    #hyper-params\n",
    "    batch_size = 5120 # increasing batch size with more gpu added\n",
    "    optim = 'Adam'\n",
    "\n",
    "    N,input_dim = X.shape\n",
    "    num_class = len(np.unique(y))\n",
    " \n",
    "    skf = StratifiedKFold(n_splits=K,random_state=SEED)\n",
    "    for fold_index, (train_index,test_index) in enumerate(skf.split(X,y)): \n",
    "        print('---------------------------------------------')\n",
    "        print('Fold #{}'.format(fold_index))    \n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        \n",
    "        X_train,y_train = balance_data(X_train,y_train,seed = SEED)\n",
    "        \n",
    "        classifier_args = (method,optim,lr,reg,batch_size,input_dim,num_class,num_epochs)\n",
    "        config =  '{}/Kfold-{}th_run/optim_{}_lr_{}_reg_{}_bs_{}'.format(method,fold_index,optim,lr,reg,batch_size)\n",
    "        runs_dir = join(dataroot,'runs',config)\n",
    "    \n",
    "        clf = getClassifier(classifier_args,runs_dir)\n",
    "        \n",
    "        clf.fit(X_train,y_train)\n",
    "    \n",
    "        pred = clf.predict(X_test,eval_mode=True)\n",
    "        test_acc = metrics.balanced_accuracy_score(y_test,pred)*100\n",
    "        print('balanced test acc: ',test_acc)\n",
    "        results[fold_index]= (test_acc)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.87\n"
     ]
    }
   ],
   "source": [
    "#neural network with 3 layer\n",
    "method = 'nn3'\n",
    "K=5\n",
    "#results = Kfold_evaluate(X,y,method=method,K=K,lr=1e-3,reg=1e-5,num_epochs=20)\n",
    "\n",
    "sum_test_acc = 0\n",
    "for fold_index,res in results.items():\n",
    "    (test_acc) = res\n",
    "    sum_test_acc+= test_acc\n",
    "print('{0:.2f}'.format(sum_test_acc/K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Fold #0\n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 0.3436\n",
      "Epoch [1/20], Step [100/398], Loss: 0.2961\n",
      "Epoch [1/20], Step [150/398], Loss: 0.2689\n",
      "Epoch [1/20], Step [200/398], Loss: 0.2282\n",
      "Epoch [1/20], Step [250/398], Loss: 0.2105\n",
      "Epoch [1/20], Step [300/398], Loss: 0.1961\n",
      "Epoch [1/20], Step [350/398], Loss: 0.1910\n",
      "Epoch [2/20], Step [1/398], Loss: 0.2380\n",
      "Epoch [2/20], Step [51/398], Loss: 0.1830\n",
      "Epoch [2/20], Step [101/398], Loss: 0.1824\n",
      "Epoch [2/20], Step [151/398], Loss: 0.1743\n",
      "Epoch [2/20], Step [201/398], Loss: 0.1796\n",
      "Epoch [2/20], Step [251/398], Loss: 0.1695\n",
      "Epoch [2/20], Step [301/398], Loss: 0.1748\n",
      "Epoch [2/20], Step [351/398], Loss: 0.1670\n",
      "Epoch [3/20], Step [2/398], Loss: 0.1799\n",
      "Epoch [3/20], Step [52/398], Loss: 0.1753\n",
      "Epoch [3/20], Step [102/398], Loss: 0.1726\n",
      "Epoch [3/20], Step [152/398], Loss: 0.1647\n",
      "Epoch [3/20], Step [202/398], Loss: 0.1824\n",
      "Epoch [3/20], Step [252/398], Loss: 0.1712\n",
      "Epoch [3/20], Step [302/398], Loss: 0.1569\n",
      "Epoch [3/20], Step [352/398], Loss: 0.1610\n",
      "Epoch [4/20], Step [3/398], Loss: 0.1922\n",
      "Epoch [4/20], Step [53/398], Loss: 0.1573\n",
      "Epoch [4/20], Step [103/398], Loss: 0.1665\n",
      "Epoch [4/20], Step [153/398], Loss: 0.1499\n",
      "Epoch [4/20], Step [203/398], Loss: 0.1718\n",
      "Epoch [4/20], Step [253/398], Loss: 0.1604\n",
      "Epoch [4/20], Step [303/398], Loss: 0.1611\n",
      "Epoch [4/20], Step [353/398], Loss: 0.1510\n",
      "Epoch [5/20], Step [4/398], Loss: 0.1810\n",
      "Epoch [5/20], Step [54/398], Loss: 0.1643\n",
      "Epoch [5/20], Step [104/398], Loss: 0.1458\n",
      "Epoch [5/20], Step [154/398], Loss: 0.1711\n",
      "Epoch [5/20], Step [204/398], Loss: 0.1582\n",
      "Epoch [5/20], Step [254/398], Loss: 0.1561\n",
      "Epoch [5/20], Step [304/398], Loss: 0.1485\n",
      "Epoch [5/20], Step [354/398], Loss: 0.1519\n",
      "Epoch [6/20], Step [5/398], Loss: 0.1774\n",
      "Epoch [6/20], Step [55/398], Loss: 0.1545\n",
      "Epoch [6/20], Step [105/398], Loss: 0.1484\n",
      "Epoch [6/20], Step [155/398], Loss: 0.1440\n",
      "Epoch [6/20], Step [205/398], Loss: 0.1420\n",
      "Epoch [6/20], Step [255/398], Loss: 0.1410\n",
      "Epoch [6/20], Step [305/398], Loss: 0.1466\n",
      "Epoch [6/20], Step [355/398], Loss: 0.1487\n",
      "Epoch [7/20], Step [6/398], Loss: 0.1369\n",
      "Epoch [7/20], Step [56/398], Loss: 0.1505\n",
      "Epoch [7/20], Step [106/398], Loss: 0.1419\n",
      "Epoch [7/20], Step [156/398], Loss: 0.1313\n",
      "Epoch [7/20], Step [206/398], Loss: 0.1318\n",
      "Epoch [7/20], Step [256/398], Loss: 0.1325\n",
      "Epoch [7/20], Step [306/398], Loss: 0.1364\n",
      "Epoch [7/20], Step [356/398], Loss: 0.1395\n",
      "Epoch [8/20], Step [7/398], Loss: 0.1358\n",
      "Epoch [8/20], Step [57/398], Loss: 0.1320\n",
      "Epoch [8/20], Step [107/398], Loss: 0.1326\n",
      "Epoch [8/20], Step [157/398], Loss: 0.1265\n",
      "Epoch [8/20], Step [207/398], Loss: 0.1162\n",
      "Epoch [8/20], Step [257/398], Loss: 0.1375\n",
      "Epoch [8/20], Step [307/398], Loss: 0.1277\n",
      "Epoch [8/20], Step [357/398], Loss: 0.1255\n",
      "Epoch [9/20], Step [8/398], Loss: 0.1293\n",
      "Epoch [9/20], Step [58/398], Loss: 0.1299\n",
      "Epoch [9/20], Step [108/398], Loss: 0.1292\n",
      "no improvement in accuracy for 10 iterations\n",
      "Loaded MachineLearningCVE/runs/cnn2/Kfold-0th_run/optim_Adam_lr_0.001_reg_1e-06_bs_5120 model trained with batch_size = 5120, seen 7 epochs and 56 mini batches\n",
      "balanced test acc:  79.69295745757134\n",
      "---------------------------------------------\n",
      "Fold #1\n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 0.3588\n",
      "Epoch [1/20], Step [100/398], Loss: 0.3063\n",
      "Epoch [1/20], Step [150/398], Loss: 0.2684\n",
      "Epoch [1/20], Step [200/398], Loss: 0.2488\n",
      "Epoch [1/20], Step [250/398], Loss: 0.2291\n",
      "Epoch [1/20], Step [300/398], Loss: 0.2334\n",
      "Epoch [1/20], Step [350/398], Loss: 0.2157\n",
      "Epoch [2/20], Step [1/398], Loss: 0.2055\n",
      "Epoch [2/20], Step [51/398], Loss: 0.2043\n",
      "Epoch [2/20], Step [101/398], Loss: 0.1778\n",
      "Epoch [2/20], Step [151/398], Loss: 0.1981\n",
      "Epoch [2/20], Step [201/398], Loss: 0.2044\n",
      "Epoch [2/20], Step [251/398], Loss: 0.1809\n",
      "Epoch [2/20], Step [301/398], Loss: 0.1957\n",
      "Epoch [2/20], Step [351/398], Loss: 0.1767\n",
      "Epoch [3/20], Step [2/398], Loss: 0.1903\n",
      "Epoch [3/20], Step [52/398], Loss: 0.1920\n",
      "Epoch [3/20], Step [102/398], Loss: 0.1836\n",
      "Epoch [3/20], Step [152/398], Loss: 0.1789\n",
      "Epoch [3/20], Step [202/398], Loss: 0.1780\n",
      "Epoch [3/20], Step [252/398], Loss: 0.1718\n",
      "Epoch [3/20], Step [302/398], Loss: 0.1720\n",
      "Epoch [3/20], Step [352/398], Loss: 0.1605\n",
      "Epoch [4/20], Step [3/398], Loss: 0.1607\n",
      "Epoch [4/20], Step [53/398], Loss: 0.1555\n",
      "Epoch [4/20], Step [103/398], Loss: 0.1726\n",
      "Epoch [4/20], Step [153/398], Loss: 0.1718\n",
      "Epoch [4/20], Step [203/398], Loss: 0.1622\n",
      "Epoch [4/20], Step [253/398], Loss: 0.1577\n",
      "Epoch [4/20], Step [303/398], Loss: 0.1606\n",
      "Epoch [4/20], Step [353/398], Loss: 0.1562\n",
      "Epoch [5/20], Step [4/398], Loss: 0.2195\n",
      "Epoch [5/20], Step [54/398], Loss: 0.1562\n",
      "Epoch [5/20], Step [104/398], Loss: 0.1539\n",
      "Epoch [5/20], Step [154/398], Loss: 0.1620\n",
      "Epoch [5/20], Step [204/398], Loss: 0.1551\n",
      "Epoch [5/20], Step [254/398], Loss: 0.1493\n",
      "Epoch [5/20], Step [304/398], Loss: 0.1526\n",
      "Epoch [5/20], Step [354/398], Loss: 0.1444\n",
      "Epoch [6/20], Step [5/398], Loss: 0.1531\n",
      "Epoch [6/20], Step [55/398], Loss: 0.1491\n",
      "Epoch [6/20], Step [105/398], Loss: 0.1385\n",
      "Epoch [6/20], Step [155/398], Loss: 0.1497\n",
      "Epoch [6/20], Step [205/398], Loss: 0.1445\n",
      "Epoch [6/20], Step [255/398], Loss: 0.1271\n",
      "Epoch [6/20], Step [305/398], Loss: 0.1353\n",
      "Epoch [6/20], Step [355/398], Loss: 0.1500\n",
      "Epoch [7/20], Step [6/398], Loss: 0.1807\n",
      "Epoch [7/20], Step [56/398], Loss: 0.1327\n",
      "Epoch [7/20], Step [106/398], Loss: 0.1389\n",
      "Epoch [7/20], Step [156/398], Loss: 0.1261\n",
      "no improvement in accuracy for 10 iterations\n",
      "Loaded MachineLearningCVE/runs/cnn2/Kfold-1th_run/optim_Adam_lr_0.001_reg_1e-06_bs_5120 model trained with batch_size = 5120, seen 5 epochs and 104 mini batches\n",
      "balanced test acc:  91.74707615817273\n",
      "---------------------------------------------\n",
      "Fold #2\n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 0.3525\n",
      "Epoch [1/20], Step [100/398], Loss: 0.2791\n",
      "Epoch [1/20], Step [150/398], Loss: 0.2605\n",
      "Epoch [1/20], Step [200/398], Loss: 0.2281\n",
      "Epoch [1/20], Step [250/398], Loss: 0.2450\n",
      "Epoch [1/20], Step [300/398], Loss: 0.2235\n",
      "Epoch [1/20], Step [350/398], Loss: 0.2107\n",
      "Epoch [2/20], Step [1/398], Loss: 0.1990\n",
      "Epoch [2/20], Step [51/398], Loss: 0.2019\n",
      "Epoch [2/20], Step [101/398], Loss: 0.1737\n",
      "Epoch [2/20], Step [151/398], Loss: 0.1828\n",
      "Epoch [2/20], Step [201/398], Loss: 0.1838\n",
      "Epoch [2/20], Step [251/398], Loss: 0.1757\n",
      "Epoch [2/20], Step [301/398], Loss: 0.1884\n",
      "Epoch [2/20], Step [351/398], Loss: 0.1726\n",
      "Epoch [3/20], Step [2/398], Loss: 0.1767\n",
      "Epoch [3/20], Step [52/398], Loss: 0.1795\n",
      "Epoch [3/20], Step [102/398], Loss: 0.1733\n",
      "Epoch [3/20], Step [152/398], Loss: 0.1769\n",
      "Epoch [3/20], Step [202/398], Loss: 0.1776\n",
      "Epoch [3/20], Step [252/398], Loss: 0.1731\n",
      "Epoch [3/20], Step [302/398], Loss: 0.1689\n",
      "Epoch [3/20], Step [352/398], Loss: 0.1605\n",
      "Epoch [4/20], Step [3/398], Loss: 0.1633\n",
      "Epoch [4/20], Step [53/398], Loss: 0.1617\n",
      "Epoch [4/20], Step [103/398], Loss: 0.1553\n",
      "Epoch [4/20], Step [153/398], Loss: 0.1539\n",
      "Epoch [4/20], Step [203/398], Loss: 0.1761\n",
      "Epoch [4/20], Step [253/398], Loss: 0.1432\n",
      "Epoch [4/20], Step [303/398], Loss: 0.1557\n",
      "Epoch [4/20], Step [353/398], Loss: 0.1517\n",
      "Epoch [5/20], Step [4/398], Loss: 0.1758\n",
      "Epoch [5/20], Step [54/398], Loss: 0.1496\n",
      "Epoch [5/20], Step [104/398], Loss: 0.1393\n",
      "Epoch [5/20], Step [154/398], Loss: 0.1373\n",
      "Epoch [5/20], Step [204/398], Loss: 0.1633\n",
      "Epoch [5/20], Step [254/398], Loss: 0.1338\n",
      "Epoch [5/20], Step [304/398], Loss: 0.1351\n",
      "Epoch [5/20], Step [354/398], Loss: 0.1428\n",
      "Epoch [6/20], Step [5/398], Loss: 0.1394\n",
      "Epoch [6/20], Step [55/398], Loss: 0.1406\n",
      "Epoch [6/20], Step [105/398], Loss: 0.1406\n",
      "Epoch [6/20], Step [155/398], Loss: 0.1361\n",
      "Epoch [6/20], Step [205/398], Loss: 0.1360\n",
      "Epoch [6/20], Step [255/398], Loss: 0.1399\n",
      "Epoch [6/20], Step [305/398], Loss: 0.1306\n",
      "Epoch [6/20], Step [355/398], Loss: 0.1265\n",
      "Epoch [7/20], Step [6/398], Loss: 0.1563\n",
      "Epoch [7/20], Step [56/398], Loss: 0.1299\n",
      "Epoch [7/20], Step [106/398], Loss: 0.1320\n",
      "Epoch [7/20], Step [156/398], Loss: 0.1237\n",
      "Epoch [7/20], Step [206/398], Loss: 0.1325\n",
      "Epoch [7/20], Step [256/398], Loss: 0.1229\n",
      "Epoch [7/20], Step [306/398], Loss: 0.1338\n",
      "Epoch [7/20], Step [356/398], Loss: 0.1257\n",
      "Epoch [8/20], Step [7/398], Loss: 0.1339\n",
      "Epoch [8/20], Step [57/398], Loss: 0.1258\n",
      "Epoch [8/20], Step [107/398], Loss: 0.1380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Step [157/398], Loss: 0.1197\n",
      "Epoch [8/20], Step [207/398], Loss: 0.1311\n",
      "Epoch [8/20], Step [257/398], Loss: 0.1200\n",
      "Epoch [8/20], Step [307/398], Loss: 0.1295\n",
      "Epoch [8/20], Step [357/398], Loss: 0.1295\n",
      "Epoch [9/20], Step [8/398], Loss: 0.1247\n",
      "Epoch [9/20], Step [58/398], Loss: 0.1225\n",
      "Epoch [9/20], Step [108/398], Loss: 0.1210\n",
      "Epoch [9/20], Step [158/398], Loss: 0.1246\n",
      "Epoch [9/20], Step [208/398], Loss: 0.1223\n",
      "Epoch [9/20], Step [258/398], Loss: 0.1230\n",
      "Epoch [9/20], Step [308/398], Loss: 0.1109\n",
      "Epoch [9/20], Step [358/398], Loss: 0.1208\n",
      "no improvement in accuracy for 10 iterations\n",
      "Loaded MachineLearningCVE/runs/cnn2/Kfold-2th_run/optim_Adam_lr_0.001_reg_1e-06_bs_5120 model trained with batch_size = 5120, seen 7 epochs and 306 mini batches\n",
      "balanced test acc:  88.39551724046086\n",
      "---------------------------------------------\n",
      "Fold #3\n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 0.3568\n",
      "Epoch [1/20], Step [100/398], Loss: 0.3144\n",
      "Epoch [1/20], Step [150/398], Loss: 0.2811\n",
      "Epoch [1/20], Step [200/398], Loss: 0.2677\n",
      "Epoch [1/20], Step [250/398], Loss: 0.2630\n",
      "Epoch [1/20], Step [300/398], Loss: 0.2273\n",
      "Epoch [1/20], Step [350/398], Loss: 0.2148\n",
      "Epoch [2/20], Step [1/398], Loss: 0.2657\n",
      "Epoch [2/20], Step [51/398], Loss: 0.2117\n",
      "Epoch [2/20], Step [101/398], Loss: 0.2096\n",
      "Epoch [2/20], Step [151/398], Loss: 0.1839\n",
      "Epoch [2/20], Step [201/398], Loss: 0.1902\n",
      "Epoch [2/20], Step [251/398], Loss: 0.1927\n",
      "Epoch [2/20], Step [301/398], Loss: 0.1907\n",
      "Epoch [2/20], Step [351/398], Loss: 0.2031\n",
      "Epoch [3/20], Step [2/398], Loss: 0.2002\n",
      "Epoch [3/20], Step [52/398], Loss: 0.1949\n",
      "Epoch [3/20], Step [102/398], Loss: 0.1842\n",
      "Epoch [3/20], Step [152/398], Loss: 0.1858\n",
      "Epoch [3/20], Step [202/398], Loss: 0.1869\n",
      "Epoch [3/20], Step [252/398], Loss: 0.1826\n",
      "Epoch [3/20], Step [302/398], Loss: 0.1830\n",
      "Epoch [3/20], Step [352/398], Loss: 0.1856\n",
      "Epoch [4/20], Step [3/398], Loss: 0.2023\n",
      "Epoch [4/20], Step [53/398], Loss: 0.1740\n",
      "Epoch [4/20], Step [103/398], Loss: 0.1885\n",
      "Epoch [4/20], Step [153/398], Loss: 0.1573\n",
      "Epoch [4/20], Step [203/398], Loss: 0.1601\n",
      "Epoch [4/20], Step [253/398], Loss: 0.1535\n",
      "Epoch [4/20], Step [303/398], Loss: 0.1622\n",
      "Epoch [4/20], Step [353/398], Loss: 0.1576\n",
      "Epoch [5/20], Step [4/398], Loss: 0.1848\n",
      "Epoch [5/20], Step [54/398], Loss: 0.1677\n",
      "Epoch [5/20], Step [104/398], Loss: 0.1524\n",
      "Epoch [5/20], Step [154/398], Loss: 0.1612\n",
      "Epoch [5/20], Step [204/398], Loss: 0.1661\n",
      "Epoch [5/20], Step [254/398], Loss: 0.1535\n",
      "Epoch [5/20], Step [304/398], Loss: 0.1550\n",
      "Epoch [5/20], Step [354/398], Loss: 0.1475\n",
      "Epoch [6/20], Step [5/398], Loss: 0.1715\n",
      "Epoch [6/20], Step [55/398], Loss: 0.1475\n",
      "Epoch [6/20], Step [105/398], Loss: 0.1502\n",
      "Epoch [6/20], Step [155/398], Loss: 0.1446\n",
      "Epoch [6/20], Step [205/398], Loss: 0.1333\n",
      "Epoch [6/20], Step [255/398], Loss: 0.1435\n",
      "Epoch [6/20], Step [305/398], Loss: 0.1602\n",
      "Epoch [6/20], Step [355/398], Loss: 0.1560\n",
      "Epoch [7/20], Step [6/398], Loss: 0.1451\n",
      "Epoch [7/20], Step [56/398], Loss: 0.1385\n",
      "Epoch [7/20], Step [106/398], Loss: 0.1364\n",
      "Epoch [7/20], Step [156/398], Loss: 0.1369\n",
      "Epoch [7/20], Step [206/398], Loss: 0.1533\n",
      "Epoch [7/20], Step [256/398], Loss: 0.1271\n",
      "Epoch [7/20], Step [306/398], Loss: 0.1511\n",
      "Epoch [7/20], Step [356/398], Loss: 0.1452\n",
      "Epoch [8/20], Step [7/398], Loss: 0.1430\n",
      "Epoch [8/20], Step [57/398], Loss: 0.1462\n",
      "Epoch [8/20], Step [107/398], Loss: 0.1305\n",
      "Epoch [8/20], Step [157/398], Loss: 0.1191\n",
      "Epoch [8/20], Step [207/398], Loss: 0.1378\n",
      "no improvement in accuracy for 10 iterations\n",
      "Loaded MachineLearningCVE/runs/cnn2/Kfold-3th_run/optim_Adam_lr_0.001_reg_1e-06_bs_5120 model trained with batch_size = 5120, seen 6 epochs and 155 mini batches\n",
      "balanced test acc:  88.27505159283271\n",
      "---------------------------------------------\n",
      "Fold #4\n",
      "best epoch 0, best batch 0\n",
      "bst acc  -1\n",
      "Epoch [1/20], Step [50/398], Loss: 0.3467\n",
      "Epoch [1/20], Step [100/398], Loss: 0.2824\n",
      "Epoch [1/20], Step [150/398], Loss: 0.2597\n",
      "Epoch [1/20], Step [200/398], Loss: 0.2318\n",
      "Epoch [1/20], Step [250/398], Loss: 0.2252\n",
      "Epoch [1/20], Step [300/398], Loss: 0.2247\n",
      "Epoch [1/20], Step [350/398], Loss: 0.2127\n",
      "Epoch [2/20], Step [1/398], Loss: 0.2075\n",
      "Epoch [2/20], Step [51/398], Loss: 0.1935\n",
      "Epoch [2/20], Step [101/398], Loss: 0.1922\n",
      "Epoch [2/20], Step [151/398], Loss: 0.1694\n",
      "Epoch [2/20], Step [201/398], Loss: 0.1840\n",
      "Epoch [2/20], Step [251/398], Loss: 0.1823\n",
      "Epoch [2/20], Step [301/398], Loss: 0.1815\n",
      "Epoch [2/20], Step [351/398], Loss: 0.1718\n",
      "Epoch [3/20], Step [2/398], Loss: 0.1814\n",
      "Epoch [3/20], Step [52/398], Loss: 0.1818\n",
      "Epoch [3/20], Step [102/398], Loss: 0.1500\n",
      "Epoch [3/20], Step [152/398], Loss: 0.1541\n",
      "Epoch [3/20], Step [202/398], Loss: 0.1646\n",
      "Epoch [3/20], Step [252/398], Loss: 0.1675\n",
      "Epoch [3/20], Step [302/398], Loss: 0.1705\n",
      "Epoch [3/20], Step [352/398], Loss: 0.1593\n",
      "Epoch [4/20], Step [3/398], Loss: 0.1754\n",
      "Epoch [4/20], Step [53/398], Loss: 0.1498\n",
      "Epoch [4/20], Step [103/398], Loss: 0.1700\n",
      "Epoch [4/20], Step [153/398], Loss: 0.1461\n",
      "Epoch [4/20], Step [203/398], Loss: 0.1662\n",
      "Epoch [4/20], Step [253/398], Loss: 0.1597\n",
      "Epoch [4/20], Step [303/398], Loss: 0.1504\n",
      "Epoch [4/20], Step [353/398], Loss: 0.1406\n",
      "Epoch [5/20], Step [4/398], Loss: 0.1781\n",
      "Epoch [5/20], Step [54/398], Loss: 0.1482\n",
      "Epoch [5/20], Step [104/398], Loss: 0.1495\n",
      "Epoch [5/20], Step [154/398], Loss: 0.1550\n",
      "Epoch [5/20], Step [204/398], Loss: 0.1451\n",
      "Epoch [5/20], Step [254/398], Loss: 0.1500\n",
      "Epoch [5/20], Step [304/398], Loss: 0.1517\n",
      "Epoch [5/20], Step [354/398], Loss: 0.1347\n",
      "Epoch [6/20], Step [5/398], Loss: 0.1678\n",
      "Epoch [6/20], Step [55/398], Loss: 0.1453\n",
      "Epoch [6/20], Step [105/398], Loss: 0.1397\n",
      "Epoch [6/20], Step [155/398], Loss: 0.1407\n",
      "Epoch [6/20], Step [205/398], Loss: 0.1436\n",
      "Epoch [6/20], Step [255/398], Loss: 0.1392\n",
      "Epoch [6/20], Step [305/398], Loss: 0.1489\n",
      "Epoch [6/20], Step [355/398], Loss: 0.1452\n",
      "Epoch [7/20], Step [6/398], Loss: 0.1450\n",
      "Epoch [7/20], Step [56/398], Loss: 0.1342\n",
      "Epoch [7/20], Step [106/398], Loss: 0.1385\n",
      "Epoch [7/20], Step [156/398], Loss: 0.1316\n",
      "Epoch [7/20], Step [206/398], Loss: 0.1379\n",
      "Epoch [7/20], Step [256/398], Loss: 0.1442\n",
      "Epoch [7/20], Step [306/398], Loss: 0.1524\n",
      "Epoch [7/20], Step [356/398], Loss: 0.1295\n",
      "Epoch [8/20], Step [7/398], Loss: 0.1351\n",
      "Epoch [8/20], Step [57/398], Loss: 0.1324\n",
      "Epoch [8/20], Step [107/398], Loss: 0.1468\n",
      "Epoch [8/20], Step [157/398], Loss: 0.1249\n",
      "Epoch [8/20], Step [207/398], Loss: 0.1303\n",
      "Epoch [8/20], Step [257/398], Loss: 0.1333\n",
      "Epoch [8/20], Step [307/398], Loss: 0.1385\n",
      "Epoch [8/20], Step [357/398], Loss: 0.1318\n",
      "Epoch [9/20], Step [8/398], Loss: 0.1296\n",
      "Epoch [9/20], Step [58/398], Loss: 0.1256\n",
      "Epoch [9/20], Step [108/398], Loss: 0.1333\n",
      "Epoch [9/20], Step [158/398], Loss: 0.1319\n",
      "Epoch [9/20], Step [208/398], Loss: 0.1204\n",
      "Epoch [9/20], Step [258/398], Loss: 0.1419\n",
      "Epoch [9/20], Step [308/398], Loss: 0.1155\n",
      "Epoch [9/20], Step [358/398], Loss: 0.1211\n",
      "Epoch [10/20], Step [9/398], Loss: 0.1385\n",
      "Epoch [10/20], Step [59/398], Loss: 0.1241\n",
      "Epoch [10/20], Step [109/398], Loss: 0.1228\n",
      "Epoch [10/20], Step [159/398], Loss: 0.1346\n",
      "Epoch [10/20], Step [209/398], Loss: 0.1172\n",
      "Epoch [10/20], Step [259/398], Loss: 0.1218\n",
      "Epoch [10/20], Step [309/398], Loss: 0.1237\n",
      "no improvement in accuracy for 10 iterations\n",
      "Loaded MachineLearningCVE/runs/cnn2/Kfold-4th_run/optim_Adam_lr_0.001_reg_1e-06_bs_5120 model trained with batch_size = 5120, seen 8 epochs and 257 mini batches\n",
      "balanced test acc:  85.45856729521385\n",
      "86.71\n"
     ]
    }
   ],
   "source": [
    "#1D Convolutional neural network with 2 conv layer, 1 dense layer\n",
    "method = 'cnn2'\n",
    "K=5\n",
    "results = Kfold_evaluate(X,y,method=method,K=K,lr=1e-3,reg=1e-6,num_epochs=20)\n",
    "\n",
    "sum_test_acc = 0\n",
    "for fold_index,res in results.items():\n",
    "    (test_acc) = res\n",
    "    sum_test_acc+= test_acc\n",
    "print('{0:.2f}'.format(sum_test_acc/K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
